{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import bs4 as bs\n",
    "import requests\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pandas import DataFrame,Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some starter notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a link\n",
    "# link = 'https://www.r-bloggers.com/getting-started-with-postgresql-in-r/'\n",
    "\n",
    "# Parse with requests library\n",
    "# r = requests.get(link)\n",
    "# print(r.content)\n",
    "\n",
    "# Alternatively access the webpage using selenium library\n",
    "# from selenium import webdriver\n",
    "# driver = webdriver.Chrome()\n",
    "# driver.get(link)\n",
    "\n",
    "# soup the link\n",
    "# soup = bs.BeautifulSoup(r.content,'lxml')\n",
    "\n",
    "# print the webpage content\n",
    "# soup\n",
    "# print(soup.prettify())\n",
    "# for x in soup.find_all(['h1','h2','h3','p']):\n",
    "#     print(x.text)\n",
    "# soup.title.text\n",
    "# for x in soup.find_all('a'):\n",
    "#     print(x.get('href')) \n",
    "# results = soup.find_all(\"span\",{\"class\":class_to_try})\n",
    "# driver.find_element_by_xpath('//*[@id=\"react-root\"]/div[1]/main/div/div/form/div/div[3]').click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(link):\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(link)\n",
    "        soup = bs.BeautifulSoup(r.content,'lxml')\n",
    "        \n",
    "        # get title\n",
    "        title = soup.title.text.strip()\n",
    "        \n",
    "        # get body\n",
    "        pre = []\n",
    "        for x in soup.find_all(['h1','h2','h3','p']):\n",
    "            pre.append(x.text)\n",
    "            body = ' '.join(pre).replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\xa0\",\" \")\n",
    "        \n",
    "        # get list with hrefs\n",
    "#         hreflist = []\n",
    "#         for x in soup.find_all('a'):\n",
    "#             hreflist.append(x.get('href'))\n",
    "\n",
    "        return [title,body] #,hreflist\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_db():\n",
    "    def __init__(self,tablename,dbname):\n",
    "        self.tablename = tablename\n",
    "        self.dbname = dbname\n",
    "        self.db = sqlite3.connect(self.dbname)\n",
    "        \n",
    "    def commit(self,df):\n",
    "        df.to_sql(self.tablename,self.db,if_exists=\"append\",index=False)\n",
    "        \n",
    "    def send_q(self,query_string):\n",
    "        self.db.execute(query_string)\n",
    "        \n",
    "    def q(self,query_string):\n",
    "        cur = self.db.cursor()\n",
    "        cur.execute(query_string)\n",
    "        return cur.fetchall()\n",
    "    \n",
    "    def get_ids(self):\n",
    "        self.idlist = [x[0] for x in db.q(\"select distinct link from marks;\")]\n",
    "        return self.idlist\n",
    "    \n",
    "    def close(self):\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commit_chunk(scrape_list,db_obj,chunk_size=10,verbose=True):\n",
    "    # initialize\n",
    "    parsed_list = []\n",
    "    title_list = []\n",
    "    body_list  = []\n",
    "    j = 1\n",
    "\n",
    "    for i,y in enumerate(scrape_list,start=1):\n",
    "        # print status\n",
    "        if verbose:\n",
    "            print(i)\n",
    "            print(f\"parsing {y}\")\n",
    "            print()\n",
    "        # parse\n",
    "        try :\n",
    "            title, body = parse(y)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        # hold in memory\n",
    "        parsed_list.append(y)\n",
    "        title_list.append(title)\n",
    "        body_list.append(body)\n",
    "        # commit to database\n",
    "        if any([len(title_list)==chunk_size,i==len(scrape_list)]):\n",
    "            # turn lists into a dict\n",
    "            temp_dc = {'link':parsed_list,\n",
    "                       'title':title_list,\n",
    "                       'body':body_list}\n",
    "            # turn dict into a df\n",
    "            temp_df = DataFrame(temp_dc)\n",
    "            # commit the df\n",
    "            db_obj.commit(temp_df)\n",
    "            # erase memory\n",
    "            parsed_list = []\n",
    "            title_list = []\n",
    "            body_list  = []\n",
    "            del temp_df\n",
    "            del temp_dc\n",
    "            # print progress\n",
    "            if verbose:\n",
    "                print(f\"commit {j} completed\")\n",
    "                print(50*'#=')\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse some bookmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* html bookmarks input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input html file containing bookmarks to scrape\n",
    "with open('bookmarks.html', encoding=\"utf8\") as html:\n",
    "    soup=bs.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "scrape_list = []\n",
    "for x in soup.find_all('a'):\n",
    "    scrape_list.append(x.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = my_db(\"marks\",\"bookmarks.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if table already exists\n",
    "db.q(\"select distinct title from marks;\")[:5]\n",
    "len(db.get_ids())\n",
    "# db.send_q(\"drop table if exists marks;\")\n",
    "\n",
    "# keep links that do not exist already\n",
    "scrape_list_new = [x for x in scrape_list if x not in db.get_ids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parse and commit to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_chunk(scrape_list_new,db,chunk_size=10,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate more data\n",
    "### _Medium_ data from kaggle\n",
    "* https://www.kaggle.com/hsankesara/medium-articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = pd.read_csv(\"./medium/articles.csv\")\n",
    "medium = med[[\"link\",\"title\",\"text\"]].drop(med.index[23]).drop(med.index[307]).reset_index().drop(\"index\",axis=1).rename(columns = {\"text\":\"body\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"bookmarks.sqlite\")\n",
    "medium.to_sql(\"med\",conn,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* merge all in one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1d65dcd7ce0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "            CREATE TABLE full AS \n",
    "            SELECT * FROM marks\n",
    "            UNION\n",
    "            SELECT * FROM med;\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDNuggets\n",
    "* Parse a local directory with html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_soup(soup):\n",
    "    \n",
    "    try:\n",
    "        title = soup.title.text.strip()\n",
    "        pre = []\n",
    "        for x in soup.find_all(['h1','h2','h3','p']):\n",
    "            pre.append(x.text)\n",
    "            body = ' '.join(pre).replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\xa0\",\" \")\n",
    "        return [title,body]\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "body_list = []\n",
    "\n",
    "setwd = \"\\\\\".join([os.getcwd(),\"projects\",\"WebScraping\",\"kdnuggets\",\"clean\"])\n",
    "os.chdir(setwd)\n",
    "\n",
    "for entry in os.listdir(os.getcwd()):\n",
    "    with open(entry, encoding=\"utf8\") as html:\n",
    "        try:\n",
    "            soup=bs.BeautifulSoup(html, 'lxml')\n",
    "            title, body = parse_soup(soup)\n",
    "\n",
    "            title_list.append(title)\n",
    "            body_list.append(body)\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "palantas = DataFrame({'title':title_list,\n",
    "                     'body':body_list})\n",
    "\n",
    "palantas['link'] = 'na'\n",
    "\n",
    "boobos = palantas[['link','title','body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"C:/Users/takis/Desktop/PyR/projects/WebScraping/bookmarks.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "boobos.to_sql(\"nuggs\",conn,if_exists=\"replace\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x27a758977a0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "            CREATE TABLE full2 AS \n",
    "            SELECT * FROM full\n",
    "            UNION\n",
    "            SELECT * FROM nuggs;\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _AnalyticsVidhya_\n",
    "* parse a local directory with html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "body_list = []\n",
    "\n",
    "where_to_look = r'C:\\Users\\takis\\Desktop\\PyR\\projects\\WebScraping\\analyticsvidhya\\clean'\n",
    "\n",
    "for entry in os.listdir(where_to_look):\n",
    "    fullnamepath = \"\\\\\".join([where_to_look,entry])\n",
    "    if not os.path.isdir(fullnamepath):\n",
    "        with open(fullnamepath, encoding=\"utf8\") as html:\n",
    "            try:\n",
    "                soup=bs.BeautifulSoup(html, 'lxml')\n",
    "                title, body = parse_soup(soup)\n",
    "\n",
    "                title_list.append(title)\n",
    "                body_list.append(body)\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = DataFrame({'title':title_list,\n",
    "                     'body':body_list})\n",
    "\n",
    "d0['link'] = 'na'\n",
    "\n",
    "d1 = d0[['link','title','body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"C:/Users/takis/Desktop/PyR/projects/WebScraping/bookmarks.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.to_sql(\"lytics\",conn,if_exists=\"replace\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check existence before commiting\n",
    "* add the word count for every article\n",
    "* parse list with href?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to parse links\n",
    "title_list = []\n",
    "body_list  = []\n",
    "parsed_list = []\n",
    "j = 1\n",
    "# href_list  = []\n",
    "\n",
    "for y in scrape_list[0:50]:\n",
    "#     title, body, hreflist = parse(y)\n",
    "    title, body = parse(y)\n",
    "\n",
    "    parsed_list.append(y)\n",
    "    title_list.append(title)\n",
    "    body_list.append(body)\n",
    "#     href_list.append(hreflist)\n",
    "#     print(title)\n",
    "\n",
    "    if len(title_list)==10:\n",
    "        print(len(title_list))\n",
    "        to_db(\"marks\",'bookmarks.sqlite',title_list,body_list)\n",
    "        title_list = []\n",
    "        body_list  = []\n",
    "        print(f\"this is the {j} commit to the db...\")\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, body = parse(scrape_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "body_list  = []\n",
    "parsed_list = []\n",
    "j = 1\n",
    "# href_list  = []\n",
    "\n",
    "for y in scrape_list[0:3]:\n",
    "#     title, body, hreflist = parse(y)\n",
    "    title, body = parse(y)\n",
    "\n",
    "    parsed_list.append(y)\n",
    "    title_list.append(title)\n",
    "    body_list.append(body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
